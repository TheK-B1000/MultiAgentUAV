# PPO / League training (train_ppo.py)
stable-baselines3>=2.0.0
gymnasium>=0.29.0
numpy>=1.20.0
torch>=1.9.0
